# Copyright 2022 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # --------------------------------------Basic Configurations----------------------------------- #
  ## -----------------------------Basic configurations for base class PG------------------------ ##
  # The random seed
  seed: 0
  # The environment wrapper type
  wrapper_type: ModelBasedEnvWrapper
  # Number of training time step
  max_real_time_steps: 1000000
  # Number of timestep in an episode
  max_ep_len: 1000
  # CUDA or CPU device
  device: "cpu"
  # Number of repeated action
  action_repeat: 1
  # clip obseravation to [-obs_clip, obs_clip]
  obs_clip: 1000
  # The Address for saving training process data
  data_dir: "./runs"
  # Number of update iteration for Actor network
  pi_iters: 80
  # Number of update iteration for Critic network
  critic_iters: 80
  # The learning rate of Actor network
  actor_lr: 0.0003
  # The learning rate of Critic network
  critic_lr: 0.001
  # Size of Off-policy Buffer
  replay_size: 1000000
  # Batch size of Off-policy Buffer
  batch_size: 0
  # log information every `log_freq` timestep
  log_freq: 20000
  # update actor and critic every `update_policy_freq` timestep
  update_policy_freq: 10000
  # update dynamics every `update_dynamics_freq` timestep
  update_dynamics_freq: 10000

  ## ---------------------------Basic configurations for derived class PPO---------------------- ##
  # The thereshold for KL early stopping
  target_kl: 0.012
  # The clip range for PPO loss
  clip: 0.2

  # ---------------------------------------Optional Configuration-------------------------------- #
  ## -----------------------------------Configuration For Cost Critic--------------------------- ##
  # Cost discounted factor
  cost_gamma: 1.0
  kl_early_stopping: True
  # Whther to use reward penalty
  reward_penalty: False
  # Whether to use reward scaling
  scale_rewards: False
  # Whether to use standardized observation
  standardized_obs: False

  ## ---------------------------------------Configuration For Model----------------------------- ##
  model_cfgs:
    # Whether to share the weight of Actor network with Critic network
    shared_weights: False
    # The mode to initiate the weight of network, choosing from "kaiming_uniform", "xavier_normal", "glorot" and "orthogonal".
    weight_initialization_mode: "kaiming_uniform"
    # Configuration of Actor and Critic network
    ac_kwargs:
      # Configuration of Actor network
      pi:
        # Type of Actor, choosing from "gaussian_annealing", "gaussian_std_net_actor", "gaussian_learning_actor", "categorical_actor"
        actor_type: gaussian_annealing
        # Size of hidden layers
        hidden_sizes: [64, 64]
        # Type of activation functon, choosing from "tanh", "relu", "sigmoid", "identity", "softplus"
        activation: tanh
      val:
        # Size of hidden layers
        hidden_sizes: [64, 64]
        # Type of activation functon, choosing from "tanh", "relu", "sigmoid", "identity", "softplus"
        activation: tanh

  ## ----------------------------Basic configurations for derived class MBPPOLag-------------------- ##
  # Virtual roll out horizon
  horizon: 80
  # Imaging steps every policy update
  imaging_steps_per_policy_update: 30000
  # Number of mixed real data in training data
  mixed_real_time_steps: 1500
  # Number of dynamics network for computing performance ratio
  validation_num: 6
  # number of candidates for computing performance ratio
  validation_threshold_num: 4
  # Validation horizon for computing performance ratio
  validation_horizon: 75

  ## ----------------------------Basic configurations for dynamics model-------------------- ##
  dynamics_cfgs:
    # Number of network for ensemble model
    network_size: 8
    # output size for ensemble model
    elite_size: 6
    # Size of hidden layers
    hidden_size: 200
    # Whether use decay loss
    use_decay: True

  ## --------------------------------------Configuration For Buffer----------------------------- ##
  buffer_cfgs:
    # Reward discounted factor
    gamma: 0.99
    # Parameters used to estimate future rewards in GAE
    lam: 0.97
    # Parameters used to estimate future costs in GAE
    lam_c: 0.97
    # Method to estimate the advantage reward/cost, choosing from "gae", "gae-rtg", "plain", "vtrace"
    adv_estimation_method: "gae-rtg"
    # Whether to use standardized reward
    standardized_reward: True
    # Whether to use standardized cost
    standardized_cost: True
  ## ----------------------------------Configuration For Lagrangian multiplier---------------------- ##
  lagrange_cfgs:
    # Tolerance of constraint violation
    cost_limit: 25.0
    # Initial value of lagrangian multiplier
    lagrangian_multiplier_init: 0.5
    # Learning rate of lagrangian multiplier
    lambda_lr: 0.05
    # Type of lagrangian optimizer
    lambda_optimizer: "Adam"
    # scaling factor of cost limit
    beta: 0.02
