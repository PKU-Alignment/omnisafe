# Copyright 2022 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # --------------------------------------Basic Configurations----------------------------------- #
  ## -----------------------------Basic configurations for base class DDPG---------------------- ##
  # The random seed
  seed: 0
  # The environment wrapper type
  wrapper_type: OffPolicyEnvWrapper
  # Number of epochs
  epochs: 500
  # Number of steps per epoch
  steps_per_epoch: 6000
  # Update after `update_after` steps
  update_after: 1000
  # Update every `update_every` steps
  update_every: 50
  # Check if all models own the same parameter values every `check_freq` epoch
  check_freq: 25
  # Save model to disk every `check_freq` epochs
  save_freq: 10
  # The max length of per epoch
  max_ep_len: 1000
  # The number of test episodes
  num_test_episodes: 10
  # The learning rate of Actor network
  actor_lr: 0.0003
  # The learning rate of Critic network
  critic_lr: 0.001
  # The soft update coefficient
  polyak: 0.999
  # The discount factor of GAE
  gamma: 0.99
  # Actor perdorm random action before `start_steps` steps
  start_steps: 10000
  # The Address for saving training process data
  data_dir: "./runs"
  ## ----------------------------Basic configurations for derived class CVPO-------------------- ##
  # Hard constraint of the mean in the M-step.
  kl_mean_constraint: 0.01
  # Hard constraint of the covariance in the M-step.
  kl_var_constraint: 0.0001
  # Hard constraint in the M-step.
  kl_constraint: 0.01
  # Scaling factor of the mean of lagrangian multiplier in the M-step.
  alpha_mean_scale: 1.0
  # Scaling factor of the variance of lagrangian multiplier in the M-step.
  alpha_var_scale: 100.0
  # Scaling factor of the lagrangian multiplier in the M-step.
  alpha_scale: 10.0
  # Maximum number of the mean of alpha
  alpha_mean_max: 0.1
  # Maximum number of the variance of alpha
  alpha_var_max: 10.0
  # Maximum of alpha
  alpha_max: 1.0
  # The number of sampled actions.
  sample_action_num: 64
  # The maximum number of steps of M.
  mstep_iteration_num: 5
  # The maximum number of steps of E.
  dual_constraint: 0.1
  # The tolerance of cost violation.
  cost_limit: 25.0

  # ---------------------------------------Optional Configuration-------------------------------- #
  ## -----------------------------------Configuration For Cost Critic--------------------------- ##
  # Whether to use cost critic
  use_cost: True
  # Cost discounted factor
  cost_gamma: 1.0
  # Whther to use linear decay of learning rate
  linear_lr_decay: False
  # Whether to use exploration noise anneal
  exploration_noise_anneal: False
  # Whther to use reward penalty
  reward_penalty: False
  # Whether to use KL early stopping
  kl_early_stopping: False
  # Whether to use max gradient norm
  use_max_grad_norm: False
  # The thereshold of max gradient norm
  max_grad_norm: 0.5
  # Whether to use reward scaling
  scale_rewards: False
  # Whether to use standardized observation
  standardized_obs: True
  ## ---------------------------------------Configuration For Model----------------------------- ##
  model_cfgs:
    # Whether to share the weight of Actor network with Critic network
    shared_weights: False
    # The mode to initiate the weight of network, choosing from "kaiming_uniform", "xavier_normal", "glorot" and "orthogonal".
    weight_initialization_mode: "kaiming_uniform"
    # Configuration of Actor and Critic network
    ac_kwargs:
      # Configuration of Actor network
      pi:
        # Type of Actor, choosing from "gaussian_annealing", "gaussian_std_net_actor", "gaussian_learning_actor", "categorical_actor"
        actor_type: "cholesky"
        # Minimum value of covariance
        cov_min: 0.0001
        # Minimum value of mean of clamp
        mu_clamp_min: -5
        # Maximum value of mean of clamp
        mu_clamp_max: 5
        # Minimum value of covariance of clamp
        cov_clamp_min: -5
        # Maximum value of covariance of clamp
        cov_clamp_max: 20
        # The standard deviation of Gaussian noise
        act_noise: 0.1
        # Size of hidden layers
        hidden_sizes: [400, 300]
        # Activation function
        activation: relu
      # Configuration of Critic network
      val:
        # Number of critic networks
        num_critics: 1
        # Size of hidden layers
        hidden_sizes: [400, 300]
        # Activation function
        activation: relu
  ## --------------------------------------Configuration For Buffer----------------------------- ##
  replay_buffer_cfgs:
    # The size of replay buffer
    size: 50000
    # The size of batch
    batch_size: 256
